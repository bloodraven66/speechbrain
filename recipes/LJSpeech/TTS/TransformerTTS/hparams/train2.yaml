###################################
# Experiment Parameters and setup #
###################################
seed: 1234
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref /home/wtc8/Sathvik/speechbrain_contrib_codes/speechbrain_samples/fastspeech_optimised_full/<seed>
save_folder: !ref <output_folder>/save
pretrained_path: !ref <save_folder>
save_for_pretrained: True
train_log: !ref <output_folder>/train_log.txt
epochs: 20000
progress_samples: True
progress_samples_incremental: False
progress_sample_path: !ref <output_folder>/samples
progress_samples_min_rin: 5
progress_samples_interval: 5
progress_batch_sample_size: 4

#################################
# Data files and pre-processing #
#################################
data_folder: ./ #data_folder: !PLACEHOLDER # e.g, /localscratch/fluent_speech_commands_dataset
duration_path: /home/wtc8/Sathvik/DeepForcedAligner/output_24hrs/durations/
data_folder_rirs: !ref <data_folder>
train_data_path: /data/Database/LJSpeech-1.1/
train_filter: null
valid_data_path: /data/Database/LJSpeech-1.1/
valid_filter: null
text_cleaners: ['english_cleaners']

################################
# Audio Parameters             #
################################
sample_rate: 16000
hop_length: 256
win_length: null
n_mel_channels: 80
n_fft: 1024
n_stft: !ref <n_fft> // 2 + 1
mel_fmin: 0.0
mel_fmax: 8000.0
power: 1
norm: "slaney"
dynamic_range_compression: True
mel_normalized: False

################################
# Optimization Hyperparameters #
################################
learning_rate: 0.0001
weight_decay: 0.000006
grad_clip_thresh: 5.0
batch_size: 48 #minimum 2
mask_padding: False

################################
# Model Parameters and model   #
################################
n_symbols: 62 #fixed deppending on symbols in textToSequence
padding_idx: 0
symbols_embedding_dim: 384

# Encoder parameters
pre_net_dropout: 0.1
pre_net_num_layers: 5

enc_num_layers: 6
enc_num_head: 1
enc_d_model: 384
enc_ffn_dim: 1536
enc_k_dim: 384
enc_v_dim: 384
enc_dropout: 0.1

# Decoder parameters
dec_num_layers: 6
dec_num_head: 1
dec_d_model: 384
dec_ffn_dim: 1536
dec_k_dim: 384
dec_v_dim: 384
dec_dropout: 0.1

# Mel-post processing network parameters
post_net_num_layers: 5

# common
normalize_before: False
ffn_type: 1dcnn #1dcnn or ffn
dur_pred_kernel_size: 3
#model
model: !new:speechbrain.lobes.models.synthesis.fastspeech.FastSpeech
    pre_net_dropout: !ref <pre_net_dropout>
    pre_net_num_layers: !ref <pre_net_num_layers>
    post_net_num_layers: !ref <post_net_num_layers>
    enc_num_layers: !ref <enc_num_layers>
    enc_num_head: !ref <enc_num_head>
    enc_d_model: !ref <enc_d_model>
    enc_ffn_dim: !ref <enc_ffn_dim>
    enc_k_dim: !ref <enc_k_dim>
    enc_v_dim: !ref <enc_v_dim>
    enc_dropout: !ref <enc_dropout>
    dec_num_layers: !ref <dec_num_layers>
    dec_num_head: !ref <dec_num_head>
    dec_d_model: !ref <dec_d_model>
    dec_ffn_dim: !ref <dec_ffn_dim>
    dec_k_dim: !ref <dec_k_dim>
    dec_v_dim: !ref <dec_v_dim>
    dec_dropout: !ref <dec_dropout>
    normalize_before: !ref <normalize_before>
    ffn_type: !ref <ffn_type>
    n_char: !ref <n_symbols>
    n_mels: !ref <n_mel_channels>
    padding_idx: !ref <padding_idx>
    dur_pred_kernel_size: !ref <dur_pred_kernel_size>

# Inverse spectrogram parameters
inverse_spectrogram_n_iter: 1024


modules:
    model: !ref <model>

#optimizer
opt_class: !name:torch.optim.Adam
    lr: !ref <learning_rate>
    weight_decay: !ref <weight_decay>

#epoch object
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <epochs>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

#annealing_function


#checkpointer
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        counter: !ref <epoch_counter>



datasets:
    train:
        path: !ref <train_data_path>
        dur_path: !ref <duration_path>
        loader: !name:recipes.LJSpeech.TTS.dataset_utils.lj.load
        filter: !ref <train_filter>
    valid:
        path: !ref <valid_data_path>
        dur_path: !ref <duration_path>
        loader: !name:recipes.LJSpeech.TTS.dataset_utils.lj.load
        filter: !ref <valid_filter>

input_encoder: !new:speechbrain.dataio.encoder.TextEncoder

model_output_keys:
    - mel
    - mel_lengths
    - alignments

pretrainer: !new:speechbrain.utils.parameter_transfer.Pretrainer
    loadables:
        model: !ref <model>
    paths:
        model: !ref <pretrained_path>/model.ckpt

encode_pipeline:
    batch: False
    output_keys:
        - text_sequences
        - input_lengths
        - durations
    steps:
        - !apply:speechbrain.lobes.models.synthesis.fastspeech.encode_text
          # TODO: Not using !ref because it triggers an odd bug when loading
          # a file from a saved model - investigate this further
          text_cleaners: ['english_cleaners']

decode_pipeline:
    steps:
        - !apply:speechbrain.lobes.models.synthesis.dataio.inverse_mel
          n_stft: !ref <n_stft>
          n_mels: !ref <n_mel_channels>
          sample_rate: !ref <sample_rate>
          f_min: !ref <mel_fmin>
          f_max: !ref <mel_fmax>
        - !apply:speechbrain.lobes.models.synthesis.dataio.inverse_spectrogram
          takes: linear
          provides: wav
          n_fft: !ref <n_fft>
          n_iter: !ref <inverse_spectrogram_n_iter>
          hop_length: !ref <hop_length>
          win_length: !ref <win_length>
