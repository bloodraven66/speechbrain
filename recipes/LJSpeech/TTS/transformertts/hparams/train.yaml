############################################################################
# Model: FastSpeech2
# Tokens: Raw characters (English text)
# Training: LJSpeech
# Authors: Sathvik Udupa
# ############################################################################

###################################
# Experiment Parameters and setup #
###################################
seed: 1234
__set_seed: !apply:torch.manual_seed [!ref <seed>]
output_folder: !ref results/transformertts_exp2/<seed>
save_folder: !ref <output_folder>/save
pretrained_path: !ref <save_folder>
save_for_pretrained: True
train_log: !ref <output_folder>/train_log.txt
epochs: 500
progress_samples: True
progress_samples_incremental: False
progress_sample_path: !ref <output_folder>/samples
progress_samples_min_run: 0
progress_samples_interval: 1
progress_batch_sample_size: 4

#################################
# Data files and pre-processing #
#################################
data_folder: /data/Database/LJSpeech-1.1 #!PLACEHOLDER # e.g., /data/Database/LJSpeech-1.1

train_json: !ref <save_folder>/train.json
valid_json: !ref <save_folder>/valid.json
test_json: !ref <save_folder>/test.json

splits: ["train", "valid"]
split_ratio: [90, 10]

skip_prep: False

text_cleaners: ['english_cleaners']

################################
# Audio Parameters             #
################################
sample_rate: 22050
hop_length: 256
win_length: null
n_mel_channels: 80
n_fft: 1024
n_stft: !ref <n_fft> // 2 + 1
mel_fmin: 0.0
mel_fmax: 8000.0
power: 1
norm: "slaney"
mel_scale: "slaney"
dynamic_range_compression: True
mel_normalized: False
min_max_energy_norm: True
min_f0: 65  #(librosa pyin values)
max_f0: 2093 #(librosa pyin values)

################################
# Optimization Hyperparameters #
################################
learning_rate: 0.0001
weight_decay: 0.000000001
grad_clip_thresh: 1.0
batch_size: 32 #minimum 2
betas: [0.98, 0.99]
################################
# Model Parameters and model   #
################################
n_symbols: 62 #fixed deppending on symbols in textToSequence
padding_idx: 0

#encoder prenet layers
enc_pre_net_symbol_embed: 512
enc_pre_net_num_layers: 3
enc_pre_net_conv_kernel_size: 3
enc_pre_net_conv_num_channels: 512

#decoder prenet layers
dec_pre_net_symbol_embed: 256
dec_pre_net_num_layers: 3

# Encoder parameters
enc_num_layers: 6
enc_num_head: 4
enc_d_model: 384
enc_ffn_dim: 1024
enc_k_dim: 384
enc_v_dim: 384
enc_dropout: 0.1

# Decoder parameters
dec_num_layers: 4
dec_num_head: 4
dec_d_model: 384
dec_ffn_dim: 1024
dec_k_dim: 384
dec_v_dim: 384
dec_dropout: 0.1

#decoder postnet
dec_post_net_num_layers: 5
dec_post_net_conv_kernel_size: 3

# common
normalize_before: True
ffn_type: 1dcnn #1dcnn or ffn

#model
model: !new:speechbrain.lobes.models.TransformerTTS.TransformerTTS
    enc_pre_net_symbol_embed: !ref <enc_pre_net_symbol_embed>
    enc_pre_net_num_layers: !ref <enc_pre_net_num_layers>
    enc_pre_net_conv_kernel_size: !ref <enc_pre_net_conv_kernel_size>
    enc_pre_net_conv_num_channels: !ref <enc_pre_net_conv_num_channels>
    dec_pre_net_symbol_embed: !ref <dec_pre_net_symbol_embed>
    dec_pre_net_num_layers: !ref <dec_pre_net_num_layers>
    dec_pre_net_in_channels: !ref <n_mel_channels>
    enc_num_layers: !ref <enc_num_layers>
    enc_num_head: !ref <enc_num_head>
    enc_d_model: !ref <enc_d_model>
    enc_ffn_dim: !ref <enc_ffn_dim>
    enc_k_dim: !ref <enc_k_dim>
    enc_v_dim: !ref <enc_v_dim>
    enc_dropout: !ref <enc_dropout>
    dec_num_layers: !ref <dec_num_layers>
    dec_num_head: !ref <dec_num_head>
    dec_d_model: !ref <dec_d_model>
    dec_ffn_dim: !ref <dec_ffn_dim>
    dec_k_dim: !ref <dec_k_dim>
    dec_v_dim: !ref <dec_v_dim>
    dec_dropout: !ref <dec_dropout>
    normalize_before: !ref <normalize_before>
    ffn_type: !ref <ffn_type>
    n_char: !ref <n_symbols>
    n_mels: !ref <n_mel_channels>
    padding_idx: !ref <padding_idx>
    dec_post_net_num_layers: !ref <dec_post_net_num_layers>
    dec_post_net_conv_kernel_size: !ref <dec_post_net_conv_kernel_size>

mel_spectogram: !name:speechbrain.lobes.models.FastSpeech2.mel_spectogram
  sample_rate: !ref <sample_rate>
  hop_length: !ref <hop_length>
  win_length: !ref <win_length>
  n_fft: !ref <n_fft>
  n_mels: !ref <n_mel_channels>
  f_min: !ref <mel_fmin>
  f_max: !ref <mel_fmax>
  power: !ref <power>
  normalized: !ref <mel_normalized>
  min_max_energy_norm: !ref <min_max_energy_norm>
  norm: !ref <norm>
  mel_scale: !ref <mel_scale>
  compression: !ref <dynamic_range_compression>
  

criterion: !new:speechbrain.lobes.models.TransformerTTS.Loss
  mel_loss_weight: 1.0

vocoder: "hifi-gan"
pretrained_vocoder: true
vocoder_source: speechbrain/tts-hifigan-ljspeech
vocoder_download_path: tmpdir_vocoder

modules:
    model: !ref <model>

train_dataloader_opts:
  batch_size: !ref <batch_size>
  drop_last: False  #True #False
  num_workers: 32
  shuffle: True
  collate_fn: !new:speechbrain.lobes.models.TransformerTTS.TextMelCollate

valid_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: 8
  shuffle: False
  collate_fn: !new:speechbrain.lobes.models.TransformerTTS.TextMelCollate

test_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: 8
  shuffle: False
  collate_fn: !new:speechbrain.lobes.models.TransformerTTS.TextMelCollate

#optimizer
opt_class: !name:torch.optim.Adam
    lr: !ref <learning_rate>
    weight_decay: !ref <weight_decay>

#epoch object
epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <epochs>

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>



#checkpointer
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
    recoverables:
        model: !ref <model>
        counter: !ref <epoch_counter>

input_encoder: !new:speechbrain.dataio.encoder.TextEncoder

model_output_keys:
    - mel
    - mel_lengths
    - alignments

progress_sample_logger: !new:speechbrain.utils.train_logger.ProgressSampleLogger
  output_path: !ref <progress_sample_path>
  batch_sample_size: !ref <progress_batch_sample_size>
  formats:
    raw_batch: raw
